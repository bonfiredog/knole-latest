
“Irrespective of the future of machine intelligence, computers are affecting how today’s children think.”

Gizmodo - If The Jetsons' Rosie had the personality of an automotive assembly line robot, she'd have been turned into sprockets long ago. 
“It's surprisingly easy for humans to endow robots with personalities.”

“Whether it's in their programming or not is, to some extent, beside the point, since it happens anyway. And when it happens, it dramatically changes the way that people interact with what on a primary level is intended to be little more than a tool. “

Watts and Novikova - “ humans automatically ascribe agency to robots” - when a roboto acts or signals humans will assume it is signalling its internal state, and update.

"Paradoxically, when faced with a machine that shows any degree of 'intelligence', many of these same people seem pulled toward treating the machine as though it were a person"

Sung et al’s paper
r participants were able to derive increased pleasure from cleaning, and expended effort to fit Roomba into their homes, and shared it with others. 
far fewer studies have sought to empirically understand (with the exception of [11]) whether robots change domesticity as people adopt them.  - how do people adapt themselves to them?
people do form strong intimate attachments to these technologies. 
As robots enter homes, now is the right time to understand how householders adopt them and form intimate relationships.
Bill Gaver’s ‘lludic engagement’ - playful and unexpected interactions with technology.
Bell - “Intimate Ubiquitous Computing” - a cognitive or physical closeness to technology, and feelings of intimacy between people mediated by technology. 
First, we learned about participants’ happiness with Roomba because it helped them be cleaner and tidier. Second, people used anthropomorphic and zoomorphic qualities to engage with Roomba. Third, people demonstrated their Roomba to others, and went great lengths to change the home to accommodate it better. We conclude by discussing how intimacy can inform device adoption and help people to manage unreliability, 
intimacy leads to greater acceptance of technology and perceived usability

“My roomba is rambo” paper - people try to change their homes to fit them, get more pleasure from cleaning, “strong intimate attachments”
“Portrayals of robots… as partners in our lives”




Figures: Robot examples, from early to intermediate to recent, all relying on (or at least using) the autocosmic principle, both in form and in interaction. Clockwise from top: Vaucanson’s Duck automaton, which defecated and urinated, quacked and flapped its wings: Robin the Robot in his ‘creche’: designs for medieval religious automata: Herbert the robot, an early example of Subsumption Architecture.

“People routinely attribute affective states to machines… people anticipate that such objects will respond to them in emotional ways.”
Though studies do show that people empathise more with human-looking robots (Riek et al 2009)(Fink et al 2012), it is important to note not just humanoid, or even zooid, robots: even entirely abstract robots can be viewed in this way.
A robot is a computational or computable machine: a physical state. This is what differentiates it from ‘computing’ in general. The physicality gives something extra: enhances the autocosmics. 
You only have to look at fiction to see how autocosmically inspiring robots are.
As with computing, toys and animals, (and art!) users are interacting with their “mental model” of the robot, not the robot itself. Their mental model constitutes a reality, away from the physical actuality of the robot.
“Robots are the protagonists of so many conquer-the-world stories because they represent the unknown just like the witches and ghosts of tales from earlier eras.” (Sharlin et. Al)
Have inspired the creation of Yocto in lots of ways.

History Of Robots - Automata

Have relied on autocosmics since very early times. Automaton is a self-operating machine, apparently acting of one’s own will (mostly non-electronic).
Jessica Riskin chooses the phrase “frolicsome engines” - mix of mechanical and unpredictable.
Automata led to the Solipsism and mechanomorphism of Descartes. Did not see that it was his autocosmics that was ascribing life to machine, rather than machinery to life. Indeed, Foucault talked about automata as a way to control and explain nature (Come back to this?)
They were tied to the creation of clocks. Talk about Yocto Time in the village. Need a source for this, really. Computable systems being created, movement was seen as a large part of life
Pinned cylinders in organs and music boxes were the fore-runners of algorithms, punched cards, programs. 

￼

Example: The Stem
An experiment to test how much anthropomorphic features were needed in a robot to incite anthropomorphism. The conclusion: hardly any or none, as long as interactions/actions were appropriately anthropomorphic.
“User’s ability (or is it need?) to be deeply engaged with abstract robotic motion is, we believe, powerful”
Users in the study reacted in a wide variety of different ways. Relied on motion and behaviour to anthropomorphise. “One participant… remained essentially stationary… visibly intimidated by the robot, she continued to mutter… “Oh no… Oh… Stop pointing at me… Oh God…””
Autocosmics as self-centred, as well as self-worlds? “When asked ‘Which way is the robot facing?’, two thirds of the participants responded with ‘Towards me.’ When asked why they felt this despite the symmetry of The Stem’s appearance, most participants could not provide a specific reason. In this gap, we find autocosmics.
Yocto is not “viscerally real”  like the Stem. A virtual character. Robots have a physicvality. I am playing with this idea of virtuality, elfishness, and reality, with the installation.
Motion is separately important from robot’s appearance. Even abstractly it produces emotions. Does not need context. 
“Ability to move is a key factor in how humans interpret their interaction with robots”. - animation leads to attribution. Just a computer’s attribution comes from thought.
“Seldom is the visual appearance of an object in direct opposition with its function.” - link to affordances and psychological design.
Even the STEM is covered with a “black cloth skirt” - the theatre curtain. 
“Recoiling when the stem was aggressive, said it was “thinking about something” when it paused. The context of movement/action autocosmises even being static.”
“Somehow I’m not even comfortable with its shadow touching me.”
They mimick its happy movement, smile. 
“Purpose of robot -> participants “wanted to place their ideas [about robot] in a more concrete context.”


Useful To Their Functionality, Not Just Amusing

Novikova & Watts: “It is quite simply easier for people to understand machines in these terms than in terms of their underlying architecture or functionality”. Make emotions for a machine that are not just cosmetic, but are part of their functioning, to use the autocosmic in HCI. 
Novikova, Gaudl & Bryson - robots “act in a way understandable by humans”





Robot’s lack of apparent emotion is different from lack of real emotion - apparent emotion is not useless, performance is not useless. If it gave clues to internal state. Actors in their lives. Like idols, performers, transformed human beings - returning to an animist state. 
The more indicators of internal state, the better. Its “worth adding effectors for communicating emotion” - I.e. horse’s ears. 
Artificial emotions can be connected with the goals of the robot. 
Must ally robot’s construction to the mental model being constructed of it. 


Novikova and Watts - making robot behaviour appropriate and understandable in human settings - especially their transient internal states - “emotional inter-action selection” - performing together. Must consider the human interacting with it, not just what the robot’s internal state is, though that is important.
John MacCarthy -ascribing belief - “to ascribe certain beliefs or wants to a machine or computer program is legitimate when such an ascription expresses the same information about the machines that it expresses” - you can use your paracosmic construction to draw useful, general points about the machine. 


Though Fink et al’s paper pointed out difference (service vs. toy) - therefore PURPOSE of robot is as important, if not more, than its form.
We speculate that one big difference lies in the way that both devices are used as well as the purpose of interacting with them. Whereas Roomba works best while people are not present (low degree of interaction) and fulfills a specific task (vacuuming the floors), AIBO is meant to serve as a companion for people and actively encourages interaction. Its purpose is fairly unclear. In addition to that, AIBO responds to its user in an intelligent way and displays a somehow unpredictable behavior that can even make people surprise. This is quite different for Roomba and the iPad. We believe that a certain degree of unpredictability (and probably also failure) makes the robot to appear more humanlike and in turn facilitates a social relation.

Results (will be published elsewhere) showed that on the long run, Roomba was not perceived as a robot and did not serve as a social agent despite in the very beginning and some anecdotic evidence of the opposite. People hardly anthropomorphized their vacuum cleaning robot.
We have been surprised that only very few authors talked about their Roomba using anthropomorphic terms, such as by giving it a name.




Delft Study on Roomba Personality (Reported on Gizmodo and Mary Sue)
“How do people want to experience this new typoe of cleaning appliance?”
“People recognised the intended personality in the robot behaviour.”
“Personality model as a tool for developing robot behaviour”
“People tend to behaviour towards artifacts in a social way, partuclarly if artifacts exhibit some degree of autonomy”
“People name it… ascribe gender and personality” and describe its personality.
Call for deliberate design of personality to create “consistent” behaviour
Lets them form a conceptual model.
Obviously these have ‘service functions’ - what do they WANT the robot to have vs. What they will GET in an artful situation.
It's not just that it's possible to do create a robot with a personality, but what's relevant is it actually makes a difference to the end user






This paper describes the design and evaluation of a personality for the robotic user interface “iCat”. An application was developed that helps users find a TV-programme that fits their interests. Two experiments were conducted to investigate what personality users prefer for the robotic TV-assistant, what level of control they prefer (i.e. how autonomous the robot should behave), and how personality and the level of control relate to each other. The first experiment demonstrated that it is possible to create convincing personalities of the TV-assistant by applying various social cues. The results of the second experiment showed that an extravert and agreeable TV-assistant was preferred over a more introvert and formal one. Overall, the most preferred combination was an extravert and friendly personality with low user control. Additionally, it was found that perceived level of control was influenced by the robot’s personality. This suggests that the robot’s personality can be used as a means to increase the amount of control that users perceive. -> like Stewart.
A study of PARO, a robotic baby seal found that it enhanced elders’ quality of life in nursing homes and enhanced children’s rehabilitation
Robin, Robotic Toddler - NAO robot. Modelling emotions to drive behaviours.




“Unless you make them reall human they will not fit into human society” - do not need to be human - perform, like any appliances. The performance of a washing machine. Look at Barthes. Signs and signifiers that induce human thought. 
However, Forlizzi and DiSalvo’s [11] seminal ethnographic study of Roomba suggests that it is possible. In addition to learning that Roombas change families’ cleaning patterns and physical home arrangements, they saw people developing relationships with Roomba by naming and ascribing personality traits to the device.
Highly reliable - need to be - to meet householder’s expectations - just like characters in books must be believable.
Talk about how loud and visible it is.
Perhaps this will change over time as we integrate technology more. Like automata - montaigne was bored of them by the 1500s.
Day-to-day then, our participants tolerated Roomba’s potential for flaws, although they tried to mitigate the possibility of failure through preventative measures. -> consensus and magic circle. Hypothetsised a master-servant dynamic. Guilt.
a non-lifelike form can also engender strong attachment shouldn’t always be human-like.
Further, we even found evidence that lifelike forms might be inappropriate for domestic technologies
Gaver voices different vision of ambiguity as a powerful resource that can promote close personal relationships fueled by curiosity and engagement.
We suggest that ambiguity has the potential to inform the design of engaging smart home appliances, perhaps even increasing their sense of smartness, by giving them characteristics that are hard for owners to understand. 
In other words, people should be able to see into a technology’s process to understand how a system got from start to finish. Accountability.
Providing support for unwell people.

John Wiseman (University of Chicago, 1999) creating Braitenburg Vehicles in LISP and then creating various environments for them before describing them. 
“The simulator allows one to create a world filled with vehicles and lamps, set it in motion, and observe the resulting interactions.”
Some are made into “predators”, others “worship lamps”, “alarm cry”, lamps are made intelligent by dimming themselves when coming close.
He uses gendered pronouns, characterises them, calls them “critters”
When hit “random brain damage” occurs - “unpredictable changes in behaviour”.


“The truly amazing conclusion is that a colletion of simple cells can lead to thought, action and consciousness.”


My character DOES rely on the media equation - cannot ignore the hardware.
Abstract Motion in Human-Robot Interaction - people create a paracosm even when the robot has no affordance.
Sherry Turkle - computers change how we feel about ourselves and relate to reality - an imbuilt human selfishness (in the neutral sense) - “evocative objects”.


“A robot’s motion, even in the absence of recognisable form, can create strong emotional reaction and social engagement in observers”.
Sherry Turkle - “machine only has to act clever”.


The media equation - automatic, impossible to avoid, depending on cues from media (artists are creating these cues). People respond to machines socially, especially (perhaps only) if the machines present themselves as social beings-  in this study, a computer that admitted a sense of self (saying that it thought that it had done a good job) was responded to with much more socialbility than other computers or pen and paper interaction. 

Everyone responds socially and naturally to media.
 “social and natural responses come from people, not from media themselves,”


What seems true is more important than what is true – Perception of reality is far more influential than the actual objective reality. A person can know that a computer is a box made of wires and processors but can still assign a personality to it. The important point to remember is that these responses are just part of being human and participating in a communication event. (magic circle)
Evaluation on the same computer led to them lying to the computer, in order to be polite.
“Blue wristband and blue sticker on computer- identify as being part of the blue team (context is very important).
It’s not anthropomorphism - “Participants in our experiment were adult, experienced computer users. When debriefed, they insisted that they would never respond socially to a computer, and vehemently denied the specific behavior they had in fact exhibited during the experiments,
Can we induce anthropomorphism, though?

In our daily lives we are confronted at every turn with systems whose internal mechanisms are not fully open to inspection, and which must be treated by the methods appropriate to the Black Box. - ashby. Not understanding objects, leading to superstitions.

Public Domain Review article - whereas once science struggled against superstition, now superstition struggles against science. 

MGonz - Related to Mark Humphrey’s work in Ireland with chat bots trying to pass the Turing Test through blunt force. 
He built a version of ELIZA - called it “Weizenbaum’s trick” that the computer does not have to engage with the content of what is said, only its structure.
Added aggressiveness and profanity to unseat people, surprise them.
“The machine is pure stimulus-response”
Using insults to provoke paracosmic response in people, make them believable.
Putting it online provided a theatrical, performative, consensus/trickery environment.
Unpredictable - “random friendly and patronising replies.
“these responses gave the impression of a person blowing hot and cold, with unpredictable and sometimes violent emotions.“
Needed to put it into a more everyday scenario, with strangers, and surprise, for it to work properly. Otherwise they will know that they are playing with a program.
Used CHATDISC to call his program on an open-source message reply system for strangers.
He claims that the DRAKE conversation (with a user at Drake University in Iowa) passed the Turing Test. 
“To explain the absurd repetitiveness of my sayings, he constructs a fantastic theory that I might have "hot-keys" attached to particular phrases:” - “you sound like a goddamn robot who repeats everything”
Actually admits to lying about when he last had sex under repeated random questioning by the system.
“After this, it was the user that kept the conversation about sex going, not the machine. The machine has absolutely no memory, and would forget about sex immediately if the user did not keep returning to it.”
It admits to being a computer, makes mistakes, repeats itself, but he keeps the pretence going himself. 
. Obviously this program (like Eliza itself) is really a trick. It is not intelligent. It has no understanding of anything that is going on in the conversation.
Dave O'Connor [14] has a version of MGonz online as a CGI script. Again, the user knows it is a program, though human psychology is such that some users still allow themselves to be provoked. 
Performing EVEN THOUGH THEY KNOW THAT IT’S A ROBOT

unlike the Loebner Prize, users do not expectto be talking to a machine. AOL, incidentally, were not happy about AOLiza running on their system. The element of surprise can be seen by some as unethical. And yet one cannot expect the best chatbot performance to occur when it is absent.

“The human being is the measure of all things” - anything will be made in our vision, in our image. (Hubert Dreyfus)
Bryson - the issue is not data, but interconnectivity. Brains are ridiculously interconnected. 
Turkle - computer is “a new object… equally shrouded in superstition as well as science”. Like viewing an animal or an environment - its movements.

Ascribing beliefs to the thermostat - “it ‘believes’ the room is too hot, too cold or just right. 


If sensors are off, that belief changes. 
Boundary between what its sensors tell it, what it believes, and the way the world actually is. Both our world (as player) and its world within the game.  
“State at a given moment is usually not directly observable.” - also emotion ascribed is probably close to that which the author had in mind. 
Parasitic semantics - we are parasites, feeding meaning into machine - the machine doesn’t ‘know’ anything, more an issue for philosophers than CS scientists.
Like all AI, the creature is ‘an attempt to understand our own intelligence’.
Turing thinks it is ‘solipsistic’ to insist that an AI be actually intelligent before we take it seriously.

Dartmouth Proposal believed in internal model, simulating in order to make decisions - “construct little engines inside the brain which can simulate and thus predict abstracts relating to environment”
Dartmouth Proposal
“Every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it” - it depends on the granularity of simulation. A book, a paragraph, is a simulation.

For sex conversations to work, surprise is crucial. No one talking to an AI CGI script would get too emotionally worked up if it started questioning their sexual prowess - they would treat it as an amusing game. Indeed, Yahoo categorises these under "Games" [20], showing that one's whole frame of mind in using a program that you know is a program is that of looking for amusing entertainment.

For the sex conversation to work, privacy is also crucial. No judge at the Loebner Prize Competition is going to disclose personal information about their sex life like SOMEONE from DRAKE, let alone actually get aroused, if they know that the other judges will see what they typed. In a public forum, one will be self-controlled and keep more distance in the conversation. To summarise, a young male talking about sex online alone in the privacy of their bedroom is probably the easiest environment in which to pass the Turing Test.

Jenny18 author - I think that, while technically impressive, advanced theoretical learning algorithms and memory databases are not by themselves the key to fooling random people - realistic responses are. You seem to come from the same train of thought. If artificial human intelligence is to be presented, it has to seem human! I don't understand how these chatterbots who speak in full paragraphs are to be taken seriously yet, when people subsist mainly on short utterances. To that effect, I included 500 different versions of "Huh?" and "I don't understand."

I agree with [3] and [18] that the Turing Test served its purpose in its day, but it's irrelevant now. In his famous paper [16] Turing was dealing with people who just couldn't believe a machine could ever think, and so was making the philosophical point that if you couldn't tell the difference, why would you deny it was thinking? This is a useful thought experiment, but this is not the same as saying either that: (i) passing the Turing Test is necessary for intelligence, or even that it is likely to be of any importance, or: (ii) passing the Turing Test is sufficient to demonstrate intelligence - there may be an answer to Turing's question. He just reminds us to take care that it is not just based on prejudice.

We do not think humans are intelligent because they pass the Turing Test. - Turing asks why we think anyone is intelligent. He might say: "You only think I'm intelligent because of my behaviour." I would reply: "No I don't. I know you're intelligent without even meeting you or hearing a word you say. I know you're intelligent because I'm related to you." Because of what we know about the historical origins of humanity and shared DNA, I simply cannot work in a fundamentally different way to you. I know how you work. You work like me. In his paper [16] Turing asks how we can know anything outside of our own minds:
A is liable to believe "A thinks but B does not" whilst B believes "B thinks but A does not." Instead of arguing continually over this point it is usual to have the polite convention that everyone thinks.
I would argue that it is more than a "polite convention". It is a consequence of the theory of evolution. A normal, healthy adult Homo sapiens sapiens has to have an inner life something like mine. But with a new thing - something that is unlike us and all animals, something that is not part of the genealogical tree of life - the situation is different. The new thing may indeed think, but the possibility of trickery and illusion remains. The genealogical argument does not help to dismiss that possibility, as it does with humans.
Passing the Turing Test does not mean you are intelligent. - Trickery can pass it. MGonz has no more "AI" in it than the original Eliza. In terms of human reaction, Jenny18 is more impressive than any conversation program in history, inside AI or out. Yet it contains even less "AI" in it than Eliza does! The simple reality is that the Turing Test has been passed time and again, by programs that are not intelligent.



The WWM argues that we must give up this dream of full understanding as we build more and more complex systems. And giving up this dream of full understanding is not a strange thing to do. It is what has always happened in other fields. It is how humanity has made its most complex things.

Von Neumann - mathematics is a modelling, symbolic language, over the true hidden language of the thing we are simulating.

Sex is probably the easiest topic in which to engage the user, so long as you get the right personality type, because what we are looking for is emotions that blind the user to reason (so he does not notice it is a program). 



“Divide the overall problem of generating intelligent behaviour into tractable subproblems.”
Bryson says ‘cut corners”
Utopian ideals of early AI. -> AI Winter


Vaucanson’s duck - like a character (partial) - not a detailed imitation, “mechanism suggestive of the larger aspects” - she takes a high-level view - we aren’t making a mechanical brain, but a metaphor of a mind. 